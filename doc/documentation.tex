\documentclass[11pt,a4paper]{article}
\usepackage[top=2in, bottom=1.5in, left=1in, right=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\title{\textbf{MEG decoding using Riemannian Geometry and Unsupervised classification.}}
\author{Alexandre Barachant \\ \\ Location : Grenoble, France \\ Email : alexandre.barachant@gmail.com }
\date{}
\newcommand{\R}{\mathbb{R}}

\newcommand{\change}{\textcolor{blue}}
\newcommand{\argmin}{\mathop{\mathrm{arg\,min}}}
\newcommand{\argmax}{\mathop{\mathrm{arg\,max}}} 
\begin{document}

\maketitle

\section{Summary}
The solution is composed by two classification steps. The first one is supervised and use data from the training subjects to build a generic model. This generic model will be applied on each test subjects to obtain a first estimation of the test labels. The second step is unsupervised and is applied independently on each test subjects. It will use the labels from the first step as an initialization of an iterative unsupervised algorithm, similar to a k-means clustering. 

These two steps are build upon methods originally devoted to classification of P300 Evoked potential for EEG Brain-Computer Interfaces (BCI) :
\begin{itemize}
\item A dedicated \textbf{Spatial filtering} is applied on the data in order to increase the signal to noise ratio and reduce the dimensionality of the signal.
\item Special form of the \textbf{covariance matrices} of the trials are used as features, and manipulated with tools from \textbf{Riemannian Geometry.} Indeed, covariance matrices are Symmetric and Positive-Definite Matrices (SPD), and therefore belong to a Riemannian manifold. A dedicated metric should be used in order to take into account the structure of the manifold.
\end{itemize}

\section{Introduction}
It is well known that EEG and MEG signals are very specific to each subject. Indeed, the organization and orientation of the cortical dipoles is slightly different for each individual. As a result, establishing a generic model, which gives high classification performance, is a very difficult task. With the exception of some special cases, a subject-specific model is always better than a generic model.

In addition, MEG signals are really noisy, and are recordings of the whole brain activity, including from parts of the brain which are not related to the task. This makes the classification task more difficult, since the unwanted activities share some common statistical properties with the signal of interest. Some of the channels contains more useful information than others, for example those around the visual cortex. However, the other channels are also useful to some degree, since they can give insights on the nature of the noise or on non-task related activity. For this reason, the MEG/EEG community has developed methods called \textbf{spatial filtering}. Spatial filtering is an operation (usually, a linear combination of the original channels) that produce virtual channels by focusing on a particular part of the brain (or a particular activity) and discarding the irrelevant informations. Those methods could be blind, i.e. based only on statistical properties of the signal of interest, or not, by using a head model and inverse solution.
Spatial filters are very specific to each subjects, and generalize badly between subjects.

For all those reasons, the wining solution should use adaptation or unsupervised training to fit the data of the test subject. Given the fact that the class are not well separated and that the signal is high dimensional, developing an efficient and stable clustering method is tricky.

During the past years, I have developed an innovative frameworks for classification of EEG signal using Riemannian Geometry~\cite{TBME,NEUROCOMP,Axiv1}. The general idea is to use covariance matrices as features for the classification, and a Riemannian metric to manipulate them. This framework features good generalization between subjects and robustness to artefacts and bad labels (among other interesting properties). It is the perfect candidate to build an efficient unsupervised method.

\section{Method}
\subsection{preprocessing}
The original data consist in MEG recording on 306 channels sampled at 250Hz. The duration of the signal is 1.5s, and a stimulus is presented at 0.5s. The first step of preprocessing is to discard the first 0.5 second of the signal, resulting in a 1s trial. The second step is to apply a 5-order Butterworth band-pass filter between 1 and 20 Hz.
\subsection{Spatial Filtering}
For each class, a set of 4 spatial filters are build in order to enhance the signal to noise ratio of the evoked potential. Therefore, the resulting signal is composed by $2\times 4 = 8 $ virtual channels. The spatial filters are estimated using an algorithm derived from the xDawn algorithm~\cite{xdawn}.
Let $\mathbf{X}_i \in \Re^{C \times N}$ denotes a trial of index $i$, with $C$ the number of channels and $N$ the number of time samples, and $y_i$ the class of this trial. Let denotes $\mathbf{P}^{(k)}$ the average trial of the class $k$ : 
\begin{equation}
\mathbf{P}^{(k)} = \frac{1}{\vert\mathcal{I}^{(k)} \vert} \sum_{i\in\mathcal{I}^{(k)}}\mathbf{X}_i,
\end{equation}
where $\mathcal{I}^{(k)}$ is the set of indices of the trials belonging to the class $k$, i.e. $\mathcal{I}^{(k)} = \lbrace i \; \vert \; y_i = k \rbrace$. Let $\mathbf{X}$ be the matrix representing the whole signal, build by concatenation of all the trials (from both classes). 

In this work, a spatial filter is a vector $\mathbf{w} \in \Re^{C \times 1}$. The spatial filter is estimated in order to increase the signal to noise ratio of a given class, i.e. for the class $k$ we have : 
\begin{equation}
\mathbf{w}^* = \argmax_{\mathbf{w}} \frac{\mathbf{w}^T \mathbf{P}^{(k)} {\mathbf{P}^{(k)}}^T \mathbf{w}}{\mathbf{w}^T\mathbf{X} \mathbf{X}^T  \mathbf{w}}.
\end{equation}
This Equation is a generalized Rayleigh quotient, the solutions can be found using an eigenvalue-eigenvector decomposition of the matrix $[(\mathbf{P}^{(k)} {\mathbf{P}^{(k)}}^T)(\mathbf{X} \mathbf{X}^T)^{-1}]$. This will give a total of $C$ solution, ranked by the value of the eigenvalues. For each class, only the 4 best spatial filters, corresponding to the 4 highest eigenvalue, are selected. 

Let denotes by $\mathbf{W}^{(k)} \in \Re^{C \times 4 }$ the selected spatial filters for the class $k$. Since we have two classes, the total number of spatial filters is 8. The spatial filters could be aggregated in a single matrix $\mathbf{W} = [ \mathbf{W}^{(0)} , \mathbf{W}^{(1)} ] \in \Re^{C \times 8}$. Then, the spatial filtering operation is simply the linear projection of the trial by the matrix $\mathbf{W}$ : 
\begin{equation}
\mathbf{Z}_i = \mathbf{W}^T \mathbf{X}_i
\end{equation}
Note that this step is supervised.
\subsection{Features}
Covariance matrices are used as feature. A special estimation of the covariance matrix is used in order to take into account the shape of the evoked potentials. First, we build a new trial $\tilde{\mathbf{Z}}_i \in \Re^{16 \times T}$ by the concatenation of the spatially filtered average evoked potential $\mathbf{P}^{(k)}$ and the spatially filtered trial $\mathbf{Z}_i$: 
\begin{equation}
\tilde{\mathbf{Z}}_i =  \left[ 
\begin{array}{c}
{\mathbf{W}^{(0)}}^T \mathbf{P}^{(0)}\\ 
{\mathbf{W}^{(1)}}^T \mathbf{P}^{(1)} \\
\mathbf{Z}_i
\end{array} \right].
\end{equation}
Then, we simply estimate the spatial covariance matrix $\mathbf{\Sigma}_i \in \Re^{16 \times 16}$ form the new trial $\tilde{\mathbf{Z}}_i$ : 

\begin{equation}
\mathbf{\Sigma}_i  = \frac{1}{N} \tilde{\mathbf{Z}}_i \tilde{\mathbf{Z}}_i^T
\end{equation}

Note that the features are not in the usual form of vectors, they are matrices. We can not simply vectorize these matrices, because we want to keep their special structure (SPD). Instead, we will use tool from Riemannian Geometry to manipulate them.

\subsection{Riemannian Geometry}
\paragraph{Riemannian Distance :}
For two covariance matrix $(\mathbf{\Sigma}_1$ and $\mathbf{\Sigma}_2) $, the Riemannian distance according to information geometry is given by~\cite{moakher_differential_2005}
\begin{equation}
\label{eq:Rgeodistance}
\delta_R(\mathbf{\Sigma}_1,\mathbf{\Sigma}_2) 
= 
\Vert \mathrm{log} \left( \mathbf{\Sigma}_1^{-1/2} \mathbf{\Sigma}_2 \mathbf{\Sigma}_1^{-1/2} \right) \Vert_F
=
\left[ \sum_{c=1}^{C} \log^2 \lambda_c \right]^{1/2},
\end{equation}
where $\lambda_c, c=1\ldots C$ are the real eigenvalues 
of $\mathbf{\Sigma}_1^{-1/2} \mathbf{\Sigma}_2 \mathbf{\Sigma}_1^{-1/2}$ and $C$ the number of electrodes.
 Note that this distance is also called \emph{Affine-invariant}~\cite{arsigny2007geometric}
\paragraph{Riemannian Mean :}
The Riemannian geometric mean of $I$ covariance matrices (denoted by $\mathfrak{G}(.)$), also called Fr\'echet mean, is defined as the matrix minimizing the sum of the squared Riemannian distances~\cite{pennec2006riemannian}, i.e.,

\begin{equation}
\mathfrak{G} \left( \mathbf{\Sigma}_1,\ldots,\mathbf{\Sigma}_I \right) = \argmin_{\mathbf{\Sigma}} 
\sum_{i=1}^{I} 
\delta_R^2 \left( \mathbf{\Sigma},\mathbf{\Sigma}_i \right).
\label{eq:geo_mean}
\end{equation}
There is no closed form expression for this mean, however a gradient descent in the manifold can be used in order to find the solution. A matlab implementation is provided in a Matlab toolbox\footnote{http://github.com/alexandrebarachant/covariancetoolbox}.

\paragraph{Tangent Space Mapping :} The tangent space mapping is an operation that project matrices from the manifold in a vector space named Tangent space. This tangent space is Euclidean and locally homomorphic to the manifold and Riemannian distance computations in the manifold can be well approximated by Euclidean distance computations in the tangent space. The idea is very similar to a kernel-mapping. 
$n\times n$ covariances matrices are represented by vectors of dimension $n(n+1)/2$ in the tangent space. For more details about how this mapping is done, please refer to~\cite{NEUROCOMP}.

\subsection{Generic Model}
A generic model is build in order to achieve the best classification accuracy in Leave-One-Subject-Out cross-validation (LOSO-CV). The test labels obtained with the generic model will be usedas an initialisation for the unsupervised classification step. This generic model is build as follow : 
\begin{itemize}
\item For each of the 16 training subjects, an set of 8 spatial filters are trained. The spatial filters are applied on the 16 subjects. Special form covariances matrices are estimated and projected in the tangent space. Thus, for each subject, we have a feature subspace of dimension [$ 136 \times $ Total Number of Trial] = $136 \times 9414$ (approximately 588 trial per subjects)
\item The 16 individual features subspace are aggregated to build a new feature space. The dimension of this feature space is $16*136 \times 9414 = 2176 \times 9414$
\item A Regularized logistic regression is trained on this feature space (function \emph{lasso }from matlab)
\end{itemize}
This generic model gives a accuracy of 69.7\% in LOSO-CV, with a public leaderboard score of 0.698 (private 0.65)

\subsection{Unsupervised training}
For each test subjects, an unsupervised algorithm is applied, using only data from the subject at hand, and initialized with the labels from the generic model. The procedure is inspired from the k-means clustering algorithm, in the Riemannian manifold. The procedure is iterative, and stop when the convergence is reached or after 10 iterations.
Lets denote by $y_{i,n}$ the label of the trial $i$ at the iteration $n$. The procedure is the following :
\begin{enumerate}
\item Train spatial filters given the labels $y_{i,n}$.
\item Apply the spatial filters on the trials.
\item Estimate special form covariance matrices.
\item For each class, estimate the mean covariance matrix from Eq.(\ref{eq:geo_mean}). We obtain two mean covariance matrix : $\mathbf{\Sigma}^{(0)}$ and $\mathbf{\Sigma}^{(1)}$.
\item Classify each trial according to the Riemannian distance from Eq.(\ref{eq:Rgeodistance}) to obtain new labels : 
\begin{equation}
 	 y_{i,(n+1)} = \left\{ \begin{array}{rl}
0 & \text{if} \:  \delta_R(\mathbf{\Sigma}^{(0)},\mathbf{\Sigma}_i) < \delta_R(\mathbf{\Sigma}^{(1)},\mathbf{\Sigma}_i)\\
1 & \text{otherwise}
\end{array} \right.
\end{equation} 
\item Stop if $y_{i,(n+1)}= y_{i,n}$ or if $n>10$. Restart to step 1 with the new labels $y_{i,(n+1)}$ if not.
\end{enumerate}

This second classification step allow to reach 75.8\% of accuracy in LOSO-CV, with a public leaderboard score of 0.774 (0.755 private). There is a risk that the algorithm does not converge for some subject. The critical part is the initialization and it is important that the generic model gives the best possible initialization.
Note that if the labels are know, this procedure could be applied in a supervised way (or semi-supervised way), with only one iteration.

\section{Results}
The results are given in Table 1. Single stands for the method described in section 3.6 applied in 6-fold CV using data of a single subject with known labels. This represent the maximum achievable accuracy if we have training data with labels for the subject at hand. Generic is the method described in section 3.5, i.e the generic model only. Gen + Unsup is the combination of 3.5 and 3.6, i.e. the winning submission. Note that for two subjects, marked with a star, the unsupervised method did not converge, leading to a decrease in accuracy.

\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c||c|c|}
\hline
Subject & Single & Generic    & Gen + Unsup     \\
\hline
1       & 87.0     & 75.5     & 64.3*   \\
2       & 82.2     & 67.5     & 72.7    \\
3       & 82.8     & 61.0     & 63.3   \\
4       & 90.7     & 75.9     & 86.5   \\
5       & 81.5     & 68.4     & 74.2   \\
6       & 83.8     & 67.6     & 73.8   \\
7       & 88.7     & 69.7     & 79.9   \\
8       & 88.8     & 68.2     & 76.6   \\
9       & 89.9     & 69.5     & 79.6   \\
10      & 86.7     & 71.5     & 78.9   \\
11      & 76.0     & 65.5     & 59.9*   \\
12      & 85.4     & 78.5     & 83.7   \\
13      & 85.8     & 68.2     & 73.4   \\
14      & 91.6     & 74.8     & 87.0   \\
15      & 91.3     & 73.9     & 87.0   \\
16      & 87.1     & 58.8     & 72.5   \\
\hline
\hline
\textbf{Mean (std) }   & \textbf{86.2 } (4.2) & \textbf{69.7} (5.3)& \textbf{75.8} (8.3) \\
\hline
public LB & NA & 69.8 & 77.8\\
private LB & NA & 65.0 & 75.5\\
\hline
\hline
\end{tabular}
\caption{Classification accuracy for the different methods. The $*$ corresponds to the subjects where the usupervised method did not converge.}
\end{center}
\end{table}

\section{Function and Code}
The code is written in Matlab and is available here : 
\begin{itemize}
\item[-] https://github.com/alexandrebarachant/DecMeg2014
\end{itemize}
The code as a dependency to my covariance toolbox, available here : 
\begin{itemize}
\item[-] https://github.com/alexandrebarachant/covariancetoolbox
\end{itemize}

There is 2 scripts and 4 functions. The scripts :
\begin{itemize}
\item \textbf{preprocessing.m} : This script reads the raw data from the data folder, applies the preprocessing steps described in section 3.1, and saves the preprocessed data in the preproc folder.
\item \textbf{final\_submission.m} : This script contains all the steps to generate the final solution.
\end{itemize}
The 4 function are : 
\begin{itemize}
\item \textbf{trainfilter.m} : This function generates the spatial filters and the averaged evoked potentials.
\item \textbf{applyfilter.m} : Apply the filters and return a spatially filtered version of the trials.
\item \textbf{extract\_features.m} : extract the features for the generic model.
\item \textbf{mdmfilter.m} : this function contains all the steps for 1 iteration of the unsupervised classification.
\end{itemize}
\section{How to generate the solution}
This code has been developed using a 16GB, intel i7-3632QM, linux laptop. The code is not optimized in terms of memory usage and can require up to 16GB of RAM. There is two dependency : my covariance toolbox (for all tge riemannian geometry related functions) and the statistical toolbox from matlab (for the function "lasso").
To generate the solution : 
\begin{enumerate}
\item Download and install the covariance toolbox\footnote{https://github.com/alexandrebarachant/covariancetoolbox}
\item Download the code\footnote{https://github.com/alexandrebarachant/DecMeg2014}
\item Place data from the competition in the "data" folder.
\item Run the preprocessing script : 'run preprocessing.m'
\item Run the final submission script : 'run final\_submission.m' 
\end{enumerate}

\section{Additional information}
As mentioned in the introduction, the best model is the model derived from the data of the subject itself. At the beginning of the competition, I focus on developing the best possible method for single subject classification. I came up pretty quickly to the solution  given in the function "mdmfilter.m". I have tried to keep thing as simple as possible to be able to implement an unsupervised procedure with this model. Then I have run some simulation to evaluate the behaviour of the unsupervised classification. It turns out that the method was robust to missclassified labels, and if you provide a good initialization (in the range of 65-70\% of accuracy), you have a very good chance of improving your results. The main challenge was to provide this good initialization. I tried several solutions, with very similar results (including more classical methods without Riemannian geometry). The generic model presented in this document was the best i could find in this limited time window. But there is room for improvement, for example by using covariate shift on the tangent space.

For the generic model, I was not very happy to use spatial filtering. Spatial filtering generalize badly between subjects, and usually leads to a loss of information. But the Riemannian geometry frameworks can't be applied with a high number of electrodes. I developed method using electrodes selection or by combining classifier on random sub-set of electrodes. One of these methods gives very good results (0.766 on the private LB), but wasn't very stable.

There is probably room for improvement, for example by applying my unsupervised procedure with an initialization provided by the methods of other competitors.
\bibliographystyle{IEEEtran}
\bibliography{biblio.bib}
\end{document}
